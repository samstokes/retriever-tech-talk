Building a Distributed Column Store for Production Observability
	Building a Distributed Column Store for Production Observability
		<IMAGE.left-half
		<bizcard.png
		>COMMENT
		>separator
		<IMAGE.right-half.force-half
		<hc_logo.svg
	Please meet Retriever
		<IMAGE.full
		<retriever.jpg
		>COMMENT
		>https://www.flickr.com/photos/rkleine/4228955783
	Please meet Retriever
		<IMAGE.left
		<retriever.jpg
		Distributed column store
		Analytic query engine
		Schemaless data model
	Please meet Retriever
		<IMAGE.left
		<retriever.jpg
		Distributed column store
		Analytic query engine
		Schemaless data model
		...
	Please meet Retriever
		<IMAGE.left
		<retriever.jpg
		Distributed column store
		Analytic query engine
		Schemaless data model
		...
		Let's back up a second...
	wat
		<IMAGE.full
		<record-scratch-dog.png
		>COMMENT
		>https://twitter.com/breadgirI/status/771607453892907009
	Retriever is a domain specific data store
		<IMAGE.left
		<retriever.jpg
		>COMMENT
		>separator
		<IMAGE.right-half.force-half
		<hc_logo.svg
	What is Honeycomb?
		<IMAGE.left-half.force-half
		<hc_logo.svg
		>MARKDOWN.incremental
		>Debugger for production
		>
		>Help engineers understand and troubleshoot distributed systems
		>
		>In between metrics and log aggregation
		>
		>...
	How Honeycomb works
		Your systems send us events
			aka structured logs
			aka JSON blobs
		<JSON
		<{
		<  "endpoint": "/dashboard",
		<  "hostname": "app32",
		<  "response_time_ms": 435,
		<  "mysql_latency_ms": 102,
		<  "status": 200,
		<  "user_id": 42
		<}
	How Honeycomb works
		We store them all
		<IMAGE.full
		<data-mode.png
	How Honeycomb works
		You query the events
		<IMAGE.full
		<query.png
	How Honeycomb works
		We turn your queries into pretty graphs
		<IMAGE.full
		<graphs.png
	Honeycomb - example
		<SQL
		<COUNT(*) WHERE status_code >= 500
		>COMMENT
		>separator
		<IMAGE
		<error-spike-1.png
		>MARKDOWN.incremental
		>Hey, what's that error spike?
		>
		>Maybe it's just one availability zone?
	Honeycomb - example
		<SQL
		<COUNT(*) WHERE status_code >= 500 GROUP BY region
		>COMMENT
		>separator
		<IMAGE
		<error-spike-2.png
		>MARKDOWN.incremental
		>Across all availability zones...
		>
		><img src="green-tick.gif" class="right" />
		>
		>Let's dig deeper
	Honeycomb - example
		<SQL
		<SELECT * WHERE status_code >= 500
		>COMMENT
		>separator
		<IMAGE.full
		<error-spike-data.png
	Honeycomb - example
		<SQL
		<COUNT(*) WHERE status_code >= 500 GROUP BY build_id
		>COMMENT
		>separator
		<IMAGE
		<error-spike-3.png
		>MARKDOWN.incremental
		>Looks like the spike came from the new build
		>
		>How widely was the bad build deployed?
	Honeycomb - example
		<SQL
		<COUNT DISTINCT(hostname) GROUP BY build_id
		>COMMENT
		>separator
		<IMAGE
		<error-spike-4.png
		Rolled out to 20% of the fleet
		Then got rolled back
	Honeycomb - example
		Other questions you might ask:
		>MARKDOWN.incremental
		>Which customers were affected by this error?
		>
		>Which customers see the highest response times?
		>
		>Which microservice was causing the error?
	Our requirements
		>MARKDOWN.incremental
		>Store lots of events
		>
		>Query them fast
		>
		><img class="right-half" src="retriever.jpg" />
	Our requirements
		>MARKDOWN.incremental
		>SQL-like queries
		>
		>BREAK DOWN and FILTER
		>
		>* on any property of the data
		>* no fixed schema or predefined indices
		>
		>High cardinality
	Our requirements
		>MARKDOWN.incremental
		>Time series and raw event data
		>
		>Operationally interesting calculations
		>
		>* percentiles, histograms
		>* COUNT_DISTINCT
		>
		>Fast!
	Our requirements
		Maintain and operate with a startup budget :)
	Our requirements
		Maintain and operate with a startup budget :)
		<IMAGE.right-half
		<rube-goldberg.jpg
		>COMMENT
		>https://www.flickr.com/photos/agrinberg/25241887726
		Simple!
			Not a general purpose database
			Constrained access patterns
			No updates
			No joins, transactions, ACID
	Where we're going
		>COMMENT
		>https://www.flickr.com/photos/oatsy40/6375359117
		<IMAGE.right-half
		<sign.jpg
		>MARKDOWN
		>Architecture Overview
		>
		>Column-oriented storage
		>
		>Distributed queries
		>
		>Operations
	Where we're going
		>COMMENT
		>https://www.flickr.com/photos/75487768@N04/16577863294
		<IMAGE.right-half
		<architecture.jpg
		>MARKDOWN
		>**Architecture Overview**
		>
		>Column-oriented storage
		>
		>Distributed queries
		>
		>Operations
	Scuba
		<IMAGE
		<scuba-header.png
		>COMMENT
		>http://db.disi.unitn.eu/pages/VLDBProgram/pdf/industry/p767-wiener.pdf
		Built to solve this problem at Facebook
		Distributed event store
	Scuba
		<IMAGE.left
		<scuba-fanout.png
		Ingest events at scale
		Store them all
			distributed across many nodes
		Fast queries by fanning out to multiple nodes
		Store everything in RAM for even faster queries
	Retriever at a glance
		<IMAGE.left-half
		<retriever.jpg
		Distributed event store
		Inspired by Scuba
		Stores raw events
			distributed across many nodes
		Fast queries by fanning out across multiple nodes
	Retriever compared to Scuba
		>MARKDOWN.incremental
		>Storage on disk
		>
		>* Much cheaper than RAM
		>* SSDs are fast
		>
		>Leverage filesystem features
		>
		>Column-oriented storage
		>
		>Uses Kafka for ingest
		>
		>* And for nice operational properties
	Architecture - write path
		>MARKDOWN
		><img class="full" src="write-path.svg" />
	Architecture - read path
		>MARKDOWN
		><img class="full" src="read-path.svg" />
	Where we're going
		>COMMENT
		>https://www.flickr.com/photos/bcymet/9097941671
		<IMAGE.right-half
		<columns.jpg
		>MARKDOWN
		>Architecture Overview
		>
		>**Column-oriented storage**
		>
		>Distributed queries
		>
		>Operations
	Data model - datasets
		Customers have one or more datasets
			analogous to tables
		Datasets are partitioned
			each dataset is assigned to a number of partitions
			typically 3, up to 39
		Dataset partitions contain events
	Data model - events
		<JSON
		<{
		<  "path": "/foo",
		<  "response_time": 142.2,
		<  "status": 200,
		<}
		>COMMENT
		>separator
		<JSON
		<{
		<  "path": "/foo",
		<  "response_time": 23,
		<  "status": 400,
		<  "error": "Bad request"
		<}
	Data model - events
		||index|timestamp|path      |response_time|status|error      |message                    |
		| 0    |45080    |/foo      |142.2        |200   |           |                           |
		| 1    |45085    |/foo      |23           |400   |Bad request|                           |
		| 2    |45087    |/bar      |657          |200   |           |                           |
		| 3    |45107    |/foo      |105          |200   |           |                           |
		| 4    |45302    |          |             |      |           |Ground control to Major Tom|
		No (fixed) schema
			Arbitrary number of fields - e.g. hundreds
			All fields are nullable
	Data model - events
		||index|timestamp|path      |response_time|status|error      |message                    |
		| 0    |45080    |/foo      |142.2        |200   |           |                           |
		| 1    |45085    |/foo      |23           |400   |Bad request|                           |
		| 2    |45087    |/bar      |657          |200   |           |                           |
		| 3    |45107    |/foo      |105          |200   |           |                           |
		| 4    |45302    |          |             |      |           |Ground control to Major Tom|
		Index is unique
			assigned on ingest
		Timestamped
	Data model - events
		||index|timestamp|path      |response_time|status|error      |message                    |
		| 0    |45080    |/foo      |142.2        |200   |           |                           |
		| 1    |45085    |/foo      |23           |400   |Bad request|                           |
		| 2    |45087    |/bar      |657          |200   |           |                           |
		| 3    |45107    |/foo      |105          |200   |           |                           |
		| 4    |45302    |          |             |      |           |Ground control to Major Tom|
		How to store events?
			Files on disk are just streams of bytes
			Row oriented?
			Column oriented?
	Row oriented storage
		||path      |response_time|status|error   |
		| /foo      |142.2        |200   |        |
		|.          |             |      |        |
		|.          |             |      |        |
		store all fields for a given record together
		||record 0|     |   | |
		|/foo     |142.2|200| |
	Row oriented storage
		||path      |response_time|status|error      |
		| /foo      |142.2        |200   |           |
		| /foo      |23           |400   |Bad request|
		|.          |             |      |           |
		store all fields for a given record together
		||record 0|     |   | |record 1|  |   |           |
		|/foo     |142.2|200| |/foo    |23|400|Bad request|
	Row oriented storage
		||path      |response_time|status|error      |
		| /foo      |142.2        |200   |           |
		| /foo      |23           |400   |Bad request|
		| /bar      |657          |200   |           |
		store all fields for a given record together
		||record 0|     |   | |record 1|  |   |           |record 2|   |   | |
		|/foo     |142.2|200| |/foo    |23|400|Bad request|/bar    |657|200| |
	Column oriented storage
		||index|timestamp|path      |response_time|status|error   |
		| 0    |45080    |/foo      |142.2        |200   |        |
		|.     |         |          |             |      |        |
		|.     |         |          |             |      |        |
		>MARKDOWN
		>path.string
		>
		>|record 0|    |
		>|--------|----|
		>|0       |/foo|
	Column oriented storage
		||index|timestamp|path      |response_time|status|error      |
		| 0    |45080    |/foo      |142.2        |200   |           |
		| 1    |45085    |/foo      |23           |400   |Bad request|
		|.     |         |          |             |      |           |
		>MARKDOWN
		>path.string
		>
		>|record 0|    |record 1|    |
		>|--------|----|--------|----|
		>|0       |/foo|1       |/foo|
	Column oriented storage
		||index|timestamp|path      |response_time|status|error      |
		| 0    |45080    |/foo      |142.2        |200   |           |
		| 1    |45085    |/foo      |23           |400   |Bad request|
		| 2    |45087    |/bar      |657          |200   |           |
		>MARKDOWN
		>path.string
		>
		>|record 0|    |record 1|    |record 2|    |
		>|--------|----|--------|----|--------|----|
		>|0       |/foo|1       |/foo|2       |/bar|
	Column oriented storage
		||index|timestamp|path      |response_time|status|error   |
		| 0    |45080    |/foo      |142.2        |200   |        |
		|.     |         |          |             |      |        |
		|.     |         |          |             |      |        |
		>MARKDOWN
		>response_time.float
		>
		>|record 0|     |
		>|--------|-----|
		>|0       |142.2|
	Column oriented storage
		||index|timestamp|path      |response_time|status|error      |
		| 0    |45080    |/foo      |142.2        |200   |           |
		| 1    |45085    |/foo      |23           |400   |Bad request|
		| .    |         |          |             |      |           |
		>MARKDOWN
		>response_time.float
		>
		>|record 0|     |record 1|    |
		>|--------|-----|--------|----|
		>|0       |142.2|1       |23  |
	Column oriented storage
		||index|timestamp|path      |response_time|status|error      |
		| 0    |45080    |/foo      |142.2        |200   |           |
		| 1    |45085    |/foo      |23           |400   |Bad request|
		| 2    |45087    |/bar      |657          |200   |           |
		>MARKDOWN
		>response_time.float
		>
		>|record 0|     |record 1|    |record 2|    |
		>|--------|-----|--------|----|--------|----|
		>|0       |142.2|1       |23  |2       |657 |
	Column oriented storage
		||index|timestamp|path      |response_time|status|error      |
		| 0    |45080    |/foo      |142.2        |200   |           |
		| .    |         |          |             |      |           |
		| .    |         |          |             |      |           |
		>MARKDOWN
		>error.string
		>
		>Don't write anything until we have a value!
	Column oriented storage
		||index|timestamp|path      |response_time|status|error      |
		| 0    |45080    |/foo      |142.2        |200   |           |
		| 1    |45085    |/foo      |23           |400   |Bad request|
		| .    |         |          |             |      |           |
		>MARKDOWN
		>error.string
		>
		>|record 1|           |
		>|--------|-----------|
		>|1       |Bad request|
	Column oriented storage
		||index|timestamp|path      |response_time|status|error      |
		| 0    |45080    |/foo      |142.2        |200   |           |
		| 1    |45085    |/foo      |23           |400   |Bad request|
		| 2    |45087    |/bar      |657          |200   |           |
		>MARKDOWN
		>error.string
		>
		>|record 1|           |
		>|--------|-----------|
		>|1       |Bad request|
	Storage format - timestamp column
		||index|timestamp|path      |response_time|status|error      |
		| 0    |45080    |/foo      |142.2        |200   |           |
		| 1    |45085    |/foo      |23           |400   |Bad request|
		| 2    |45087    |/bar      |657          |200   |           |
		>MARKDOWN
		>Special "timestamp" column always present
		>
		>|record 0|     |record 1|     |record 2|     |record 3...|
		>|--------|-----|--------|-----|--------|-----|-----------|
		>|0       |45808|1       |45085|2       |45087|...        |
		>
		>Tells us what index values exist
		>
		>Let us filter by timestamp
	Storage format - reading
		<IMAGE.left-half
		<reading.jpg
		>COMMENT
		>downloaded from
		>https://www.flickr.com/photos/triviaqueen/95609350
		How do we read column-oriented data?
	Storage format - reading
		<IMAGE.left-half
		<reading.jpg
		Find out what columns exist
	Storage format - reading
		<IMAGE.left-half
		<reading.jpg
		Find out what columns exist
		Columns are just files in a directory
			just list the directory contents
	Storage format - reading
		<IMAGE.left-half
		<reading.jpg
		Find out what columns exist
		Columns are just files in a directory
			just list the directory contents
		;$ ls
		;path.string
		;response_time.float
		;status.int
		;error.string
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		open the column files we need
			index (from timestamp column)
			status (for filter)
			response_time
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		open the column files we need
		index
		|*|
		status.int
		|*|
		response_time.float
		|*|
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		read an index
		index
		|0       |*|
		status.int
		|*|
		response_time.float
		|*|
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		read from status file until we hit index 0
		index
		|0       |*|
		status.int
		|0  | 200 |*|
		response_time.float
		|*|
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		status == 200!
		index
		|0       |*|
		status.int
		|0  | 200 |*|
		response_time.float
		|*|
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		read from response_time file until we hit index 0
		index
		|0       |*|
		status.int
		|0  | 200 |*|
		response_time.float
		|0 | 142.2 |*|
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		collect response_time
		index
		|0       |*|
		status.int
		|0  | 200 |*|
		response_time.float
		|0 | 142.2 |*|
		response_times: [142.2]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		read an index
		index
		|0       |1|*|
		status.int
		|0  | 200 |*|
		response_time.float
		|0 | 142.2 |*|
		response_times: [142.2]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		read from status file until we hit index 1
		index
		|0       |1|*|
		status.int
		|0  | 200 |1  | 400 |*|
		response_time.float
		|0 | 142.2 |*|
		response_times: [142.2]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		status ≠ 200, skip this event!
		index
		|0       |1|*|
		status.int
		|0  | 200 |1  | 400 |*|
		response_time.float
		|0 | 142.2 |*|
		response_times: [142.2]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		read an index
		index
		|0       |1|2|*|
		status.int
		|0  | 200 |1  | 400 |*|
		response_time.float
		|0 | 142.2 |*|
		response_times: [142.2]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		read from status file until we hit index 2
		index
		|0       |1|2|*|
		status.int
		|0  | 200 |1  | 400 |2|200|*|
		response_time.float
		|0 | 142.2 |*|
		response_times: [142.2]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		status == 200!
		index
		|0       |1|2|*|
		status.int
		|0  | 200 |1  | 400 |2|200|*|
		response_time.float
		|0 | 142.2 |*|
		response_times: [142.2]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		read from response_time file until we hit index 2
		index
		|0       |1|2|*|
		status.int
		|0  | 200 |1  | 400 |2|200|*|
		response_time.float
		|0 | 142.2 |1 | 23 |*|
		response_times: [142.2]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		read from response_time file until we hit index 2
		index
		|0       |1|2|*|
		status.int
		|0  | 200 |1  | 400 |2|200|*|
		response_time.float
		|0 | 142.2 |1 | 23 |2 | 657 |*|
		response_times: [142.2]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		collect response_time
		index
		|0       |1|2|*|
		status.int
		|0  | 200 |1  | 400 |2|200|*|
		response_time.float
		|0 | 142.2 |1 | 23 |2 | 657 |*|
		response_times: [142.2, 657]
	Storage format - reading
		e.g. "AVG(response_time) WHERE status = 200"
		etc
		index
		|0       |1       |2       |*|
		status.int
		|0  | 200 | 1 | 400 | 2 | 200 |*|
		response_time.float
		|0       |142.2|1       |23  |2       |657 |*|
		response_times: [142.2, 657]
	Storage format - reading
		>MARKDOWN
		> --------- ---------- ----------------- ---------- -----------
		> **index** path       **response_time** **status** error    
		> **0**     /foo         **142.2**       **200**           
		> **1**     /foo         23              **400**    Bad request 
		> **2**     /bar         **657**         **200**             
		> --------- ---------- ----------------- ---------- -----------
		>
		>Table: only values in **bold** get read
		e.g. "AVG(response_time) WHERE status = 200"
		Only read what you need!
			didn't touch other columns
	Where we're going
		<IMAGE.right-half
		<http://cs.umw.edu/~finlayson/class/fall12/cpsc230/notes/images/tree.png
		>MARKDOWN
		>Architecture Overview
		>
		>Column-oriented storage
		>
		>**Distributed queries**
		>
		>Operations
	Distributed queries
		<IMAGE.left-half
		<read-path-zoomed.svg
		>MARKDOWN.incremental
		>Client issues a query to a retriever root node
		>
		>Root retriever forwards the query to retrievers on other partitions
		>
		>* All scan rows in parallel
		>* All perform local calculations
		>* All return calculations to root node
		>
		>Root retriever merges results and returns to client
	Distributed reads - calculations
		Data is partitioned across nodes
		So each node can only do part of the calculation
		Need to be careful about combining results
	Distributed reads - calculations
		Data is partitioned across nodes
		So each node can only do part of the calculation
		Need to be careful about combining results
		<RUBY
		<# e.g. averaging two averages gives the wrong answer
		<AVG( 1, 2, 3, 3 ) # => 2.25
		<AVG( AVG( 1, 2, 3 ), AVG( 3 ) ) # => 2.5
	Distributed reads - calculations
		Data is partitioned across nodes
		So each node can only do part of the calculation
		Need to be careful about combining results
		<RUBY
		<# e.g. averaging two averages gives the wrong answer
		<AVG( 1, 2, 3, 3 ) # => 2.25
		<AVG( AVG( 1, 2, 3 ), AVG( 3 ) ) # => 2.5
		Send back partial results that can be combined
		<RUBY
		<# e.g. partial counts and sums can be combined correctly
		<SUM( 1, 2, 3, 3 ) / 4 # => 2.25
		<(SUM( 1, 2, 3 ) + SUM( 3 )) / (3 + 1) # => 2.25
	Distributed reads - calculations
		Data is partitioned across nodes
		So each node can only do part of the calculation
		Other partial results that can be combined:
	Distributed reads - calculations
		Data is partitioned across nodes
		So each node can only do part of the calculation
		Other partial results that can be combined:
		Groups
		<RUBY
		<{"/dashboard": 235, "/products/iphone": 454}
	Distributed reads - calculations
		Data is partitioned across nodes
		So each node can only do part of the calculation
		Other partial results that can be combined:
		Groups
		<RUBY
		<{"/dashboard": 235, "/products/iphone": 454}
		COUNT DISTINCT
			HyperLogLog
	Distributed reads - calculations
		Data is partitioned across nodes
		So each node can only do part of the calculation
		Other partial results that can be combined:
		Groups
		<RUBY
		<{"/dashboard": 235, "/products/iphone": 454}
		COUNT DISTINCT
			HyperLogLog
		Percentiles
			T-digest
	Distributed reads - fanout
		Root node merges the results
		May still have to do a lot of work
			e.g. merging large numbers of groups
		Don't want to overwhelm the root
	Distributed reads - fanout
		<IMAGE.full
		<read-path-recursive.svg
	Where we're going
		<IMAGE.right-half
		<devops.jpg
		>MARKDOWN
		>Architecture Overview
		>
		>Column-oriented storage
		>
		>Distributed queries
		>
		>**Operations**
	Detour - Kafka
		<IMAGE
		<kafka.png
		Retriever relies on Kafka for ingesting events
		Gives us:
			Write distribution
			Replication
			Fault tolerance
			Disaster recovery
	Detour - Kafka
		<IMAGE
		<kafka.png
		Kafka is a distributed log
			~ message queue
		Publish messages to topics
			~ tables
		Topics are partitioned
			horizontal scaling 
		>MARKDOWN
		>Messages *within a partition* are totally ordered
	Detour - Kafka
		<IMAGE
		<kafka.png
		Kafka actually stores messages on disk
			whether or not anyone is consuming them
			unlike most message queues
		Allows multiple consumers
			aka pub-sub
		Allows replaying
	Ingestion
		Clients publish events to a Kafka topic
			Kafka topic is partitioned
			Datasets are assigned to partitions
		Client chooses which partition to write to
			Client checks partition assignment for dataset
			Picks a partition (at random)
		Retriever on that partition consumes events from Kafka
			and writes to disk
		All writes replicated to two nodes
			Each partition of the Kafka topic has two retrievers consuming it
	Quota management
		<IMAGE.left-half
		<dam.jpg
		>COMMENT
		>https://www.flickr.com/photos/nevilleslens/15957038008
		Each customer gets a storage quota
		Want to age out old data past quota
	Quota management
		<IMAGE.left-half
		<dam.jpg
		Split events into segments
			Segments are just directories on disk
			Start a new segment when we've written enough events
		Calculate space occupied by each segment
			Just stat the files!
		Background job periodically deletes oldest data
			Just delete the directories!
	Fault tolerance
		What if retriever goes down?
			Crash, network outage...
			Deploy / planned maintenance
		We have two replicas...
	Fault tolerance
		What if retriever goes down?
			Crash, network outage...
			Deploy / planned maintenance
		We have two replicas...
		But we don't want to miss events coming in
	Failure recovery
		Each retriever tracks Kafka offset
			Events are totally ordered in Kafka (per partition)
		On boot, reconsume all events since last offset
	Failure recovery
		Periodic checkpoints
			Store Kafka offset of last-written message
			Store *index* of last-written message
		Determines where to reconsume from
	Failure recovery
		Periodic checkpoints
			Store Kafka offset of last-written message
			Store *index* of last-written message
		Determines where to reconsume from
		Truncate written data to avoid duplicate writes
			events up to checkpoint index was committed
			anything after that is suspect
	Bootstrapping new nodes
		What if a node disappears completely?
		Find an existing node on the same partition
		Copy over the data
			just rsync the directory structure!
		... then consume Kafka from last checkpoint
	Operations - summary
		Replication
			via Kafka
		Fault tolerance
			via Kafka
		Quota management
			via filesystem
		Bootstrapping new nodes
			via rsync
			and Kafka
	Retriever
		<IMAGE.full
		<retriever.jpg
	Summary
		Column-oriented storage is a cool trick
			only read what you need
	Summary
		Column-oriented storage is a cool trick
			only read what you need
		Kafka solves distributed systems problems for you
			fault tolerance
			replication
	Summary
		Column-oriented storage is a cool trick
			only read what you need
		Kafka solves distributed systems problems for you
			fault tolerance
			replication
		Filesystems are actually pretty useful
			read caching
			atomic renames
			rsync!
	Summary
		Column-oriented storage is a cool trick
			only read what you need
		Kafka solves distributed systems problems for you
			fault tolerance
			replication
		Filesystems are actually pretty useful
			read caching
			atomic renames
			rsync!
		Look for ways to make hard problems easy
	Credits
		<IMAGE.right-half
		<bizcard.png
		>MARKDOWN
		> * retriever - rkleine (Flickr)
		> * record scratch dog - breadgirI (Twitter)
		> * architecture - barnyz (Flickr)
		> * Scuba paper - Facebook (various authors)
		> * columns - bcymet (Flickr)
		> * reading - triviaqueen (Flickr)
		> * dam - nevilleslens (Flickr)
		> * rube goldberg machine - agrinberg (Flickr)
	Fin
	Slide graveyard for longer versions of this talk
		Abandon hope, all ye who enter here
	Write path
		POST event to shepherd
			Choose a partition
		Enqueue "retriever mutation" to Kafka partition
		Retrievers consume mutations and write to disk
			Two retrievers per partition
	Write path - details
		Retrievers receives a row from Kafka
		Assign an index
		Open an appender for each column
			basically a file descriptor
		Write values in row to each column appender
		Hold appender open in case more rows come in for same dataset
	Read path - query execution
		2-stage: gather all rows, run calculations
		Gather:
			Always scan timestamp column (consider every row)
			FILTER:
				for each index in timestamp column
				seek to corresponding index in filter column
				skip index if filter fails
		Accumulate:
			COUNT, AVG, quantiles, etc
			BREAKDOWN
	Read path - node selection
		checks which partitions the dataset is assigned to
		checks Kennel for retriever nodes on those partitions
		picks one node as the root
	Storage format - strings
		Variable-length strings
		Stored with length prefix
			||index|len|value|
			| 0    |3  |The  |
			| 1    |5  |quick|
			| 3    |5  |brown|
			| 7    |3  |fox  |
		Separate index file to support seek
			||index|len|offset|
			| 0    |3  |0     |
			| 1    |5  |11    |
			| 3    |5  |24    |
			| 7    |3  |35    |
